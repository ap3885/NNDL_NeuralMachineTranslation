{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Neural Machine Translation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of neural machine translation(NMT) implements multi-layer Recurrent Neural Network (RNN, LSTM, and GRU) for translating from one language to another. \n",
    "\n",
    "Here the task is to train a neural machine translation RNN in tensorflow to translate from french to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anany\\Anaconda3\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\anany\\Anaconda3\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\anany\\Anaconda3\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\anany\\Anaconda3\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\anany\\Anaconda3\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\anany\\Anaconda3\\envs\\envTF113\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from utils.nmt import *\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup \n",
    "\n",
    "Here will will preprocess the data necessary for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'nmt_data/fr.txt'\n",
    "target_path = 'nmt_data/en.txt'\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at some sample translation to get an inderstanding of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations: \n",
      "\n",
      "sentence number 1\n",
      "\tfr: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\ten: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "sentence number 2\n",
      "\tfr: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\ten: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\n",
      "sentence number 3\n",
      "\tfr: california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\ten: california is usually quiet during march , and it is usually hot in june .\n",
      "\n",
      "sentence number 4\n",
      "\tfr: les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "\ten: the united states is sometimes mild during june , and it is cold in september .\n",
      "\n",
      "sentence number 5\n",
      "\tfr: votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "\ten: your least liked fruit is the grape , but my least liked is the apple .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_sentences = target_text.split('\\n')\n",
    "french_sentences = source_text.split('\\n')\n",
    "\n",
    "#Fetch the first five translations\n",
    "sents = list(zip(french_sentences,english_sentences))[0:5]\n",
    "\n",
    "side_by_side_sentences = list(zip(english_sentences, french_sentences))[0:5]\n",
    "print(\"Sample translations: \\n\")\n",
    "for index, sentence in enumerate(side_by_side_sentences):\n",
    "    en_sent,fr_sent  = sentence\n",
    "    print('sentence number {}'.format(index+1))\n",
    "    print('\\tfr: {}'.format(fr_sent))\n",
    "    print('\\ten: {}'.format(en_sent))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the translation data. We will do the following preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create lookup tables\n",
    "    - here we create a unique mapping between each distinct word and it's word id\n",
    "2. text to word ids\n",
    "    - we convert all the text sentences to word id replaced sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#please look at utils/nmt.py for the full function\n",
    "preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data after it has been preprocessed\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create the RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/seq2seq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq2seq learning model in this assignment is based on this paper: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translation model can be visualized in the simplest way as shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions to look up for seq to seq translation task are\n",
    "\n",
    "\n",
    "\n",
    "#### Encoder\n",
    "- [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence)\n",
    "\n",
    "RNN layers\n",
    "- [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell)\n",
    "  - creates an LSTM cell\n",
    "- [`tf.contrib.rnn.GRUCell`](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/GRUCell)\n",
    "  - creates an LSTM cell\n",
    "- [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper)\n",
    "  - wraps a cell with keep probability value \n",
    "- [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell)\n",
    "  - stacks multiple RNN (type) cells\n",
    "  \n",
    "Encoding model\n",
    "- [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    "  - put Embedding layer and RNN layer(s) all together\n",
    "\n",
    "#### Decoder training\n",
    "- [`tf.contrib.seq2seq.TrainingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper)\n",
    "  - TrainingHelper is where we pass the embeded input. As the name indicates, this is only a helper instance. This instance should be delivered to the BasicDecoder, which is the actual process of building the decoder model.\n",
    "- [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n",
    "  - BasicDecoder builds the decoder model. It means it connects the RNN layer(s) on the decoder side and the input prepared by TrainingHelper.\n",
    "- [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)\n",
    "  - dynamic_decode unrolls the decoder model so that actual prediction can be retrieved by BasicDecoder for each time steps.\n",
    "  \n",
    "#### Decoder inference\n",
    "- [`tf.contrib.seq2seq.GreedyEmbeddingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper)\n",
    "  - GreedyEmbeddingHelper dynamically takes the output of the current step and give it to the next time step's input. In order to embed the each input result dynamically, embedding parameter(just bunch of weight values) should be provided. Along with it, GreedyEmbeddingHelper asks to give the `start_of_sequence_id` for the same amount as the batch size and `end_of_sequence_id`.\n",
    "- [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n",
    "  - same as described in the training process section\n",
    "- [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)\n",
    "  - same as described in the training process section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use 4 special tokens for translation:\n",
    "\n",
    "    * <PAD>: to pad the sentence so all sentences are of the same length\n",
    "    * <EOS>: to mark the end of sentence\n",
    "    * <UNK>: to mark texts which are not in our dictionary\n",
    "    * <GO>: the first token that is passed to the decoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span>: finish the following functions in utils/nmt.py. Refer to the functions above on which functions could be helpful\n",
    "\n",
    "    * encoding_layer : creates the enncoder part of the seq-seq learning architecture.\n",
    "    * decoding_layer : creates the decoder part of the seq2seq learning architecture. This function outputs both the output during training and output during inference\n",
    "    * my_optimizer : implements the optimizer with gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we will create a Seq2Seq model using LSTM as the building block. First we will will define all the hyperparameters required. Feel free to play around with them to improve the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "display_step = 300\n",
    "\n",
    "epochs = 13\n",
    "batch_size = 128\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5\n",
    "grad_clip = 5\n",
    "cell_type = 'LSTM'\n",
    "rnn1_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define the graph for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev_LSTM'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int,\n",
    "                                                   cell_type)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        train_op = my_optimizer(cost,grad_clip,lr)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the graph has been defined we will train the seq2seq model to perform the translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  300/1077 - Train Accuracy: 0.5582, Validation Accuracy: 0.5386, Loss: 1.2384\n",
      "Epoch   0 Batch  600/1077 - Train Accuracy: 0.7036, Validation Accuracy: 0.7053, Loss: 0.8123\n",
      "Epoch   0 Batch  900/1077 - Train Accuracy: 0.7161, Validation Accuracy: 0.7582, Loss: 0.7334\n",
      "Epoch   1 Batch  300/1077 - Train Accuracy: 0.7912, Validation Accuracy: 0.7990, Loss: 0.5973\n",
      "Epoch   1 Batch  600/1077 - Train Accuracy: 0.7964, Validation Accuracy: 0.8073, Loss: 0.5428\n",
      "Epoch   1 Batch  900/1077 - Train Accuracy: 0.8051, Validation Accuracy: 0.8147, Loss: 0.5431\n",
      "Epoch   2 Batch  300/1077 - Train Accuracy: 0.8116, Validation Accuracy: 0.8160, Loss: 0.4859\n",
      "Epoch   2 Batch  600/1077 - Train Accuracy: 0.8286, Validation Accuracy: 0.8329, Loss: 0.4491\n",
      "Epoch   2 Batch  900/1077 - Train Accuracy: 0.8307, Validation Accuracy: 0.8394, Loss: 0.4512\n",
      "Epoch   3 Batch  300/1077 - Train Accuracy: 0.8624, Validation Accuracy: 0.8602, Loss: 0.3831\n",
      "Epoch   3 Batch  600/1077 - Train Accuracy: 0.8698, Validation Accuracy: 0.8694, Loss: 0.3463\n",
      "Epoch   3 Batch  900/1077 - Train Accuracy: 0.8720, Validation Accuracy: 0.8772, Loss: 0.3475\n",
      "Epoch   4 Batch  300/1077 - Train Accuracy: 0.9015, Validation Accuracy: 0.8937, Loss: 0.2871\n",
      "Epoch   4 Batch  600/1077 - Train Accuracy: 0.9110, Validation Accuracy: 0.9023, Loss: 0.2524\n",
      "Epoch   4 Batch  900/1077 - Train Accuracy: 0.9110, Validation Accuracy: 0.9080, Loss: 0.2541\n",
      "Epoch   5 Batch  300/1077 - Train Accuracy: 0.9319, Validation Accuracy: 0.9158, Loss: 0.2074\n",
      "Epoch   5 Batch  600/1077 - Train Accuracy: 0.9314, Validation Accuracy: 0.9258, Loss: 0.1896\n",
      "Epoch   5 Batch  900/1077 - Train Accuracy: 0.9332, Validation Accuracy: 0.9271, Loss: 0.1943\n",
      "Epoch   6 Batch  300/1077 - Train Accuracy: 0.9436, Validation Accuracy: 0.9323, Loss: 0.1516\n",
      "Epoch   6 Batch  600/1077 - Train Accuracy: 0.9497, Validation Accuracy: 0.9470, Loss: 0.1349\n",
      "Epoch   6 Batch  900/1077 - Train Accuracy: 0.9466, Validation Accuracy: 0.9484, Loss: 0.1437\n",
      "Epoch   7 Batch  300/1077 - Train Accuracy: 0.9592, Validation Accuracy: 0.9518, Loss: 0.1095\n",
      "Epoch   7 Batch  600/1077 - Train Accuracy: 0.9575, Validation Accuracy: 0.9536, Loss: 0.1059\n",
      "Epoch   7 Batch  900/1077 - Train Accuracy: 0.9549, Validation Accuracy: 0.9583, Loss: 0.1128\n",
      "Epoch   8 Batch  300/1077 - Train Accuracy: 0.9718, Validation Accuracy: 0.9679, Loss: 0.0777\n",
      "Epoch   8 Batch  600/1077 - Train Accuracy: 0.9718, Validation Accuracy: 0.9735, Loss: 0.0790\n",
      "Epoch   8 Batch  900/1077 - Train Accuracy: 0.9696, Validation Accuracy: 0.9735, Loss: 0.0757\n",
      "Epoch   9 Batch  300/1077 - Train Accuracy: 0.9770, Validation Accuracy: 0.9714, Loss: 0.0639\n",
      "Epoch   9 Batch  600/1077 - Train Accuracy: 0.9757, Validation Accuracy: 0.9735, Loss: 0.0529\n",
      "Epoch   9 Batch  900/1077 - Train Accuracy: 0.9779, Validation Accuracy: 0.9774, Loss: 0.0575\n",
      "Epoch  10 Batch  300/1077 - Train Accuracy: 0.9774, Validation Accuracy: 0.9740, Loss: 0.0462\n",
      "Epoch  10 Batch  600/1077 - Train Accuracy: 0.9753, Validation Accuracy: 0.9800, Loss: 0.0435\n",
      "Epoch  10 Batch  900/1077 - Train Accuracy: 0.9783, Validation Accuracy: 0.9766, Loss: 0.0501\n",
      "Epoch  11 Batch  300/1077 - Train Accuracy: 0.9783, Validation Accuracy: 0.9753, Loss: 0.0404\n",
      "Epoch  11 Batch  600/1077 - Train Accuracy: 0.9774, Validation Accuracy: 0.9809, Loss: 0.0405\n",
      "Epoch  11 Batch  900/1077 - Train Accuracy: 0.9779, Validation Accuracy: 0.9805, Loss: 0.0490\n",
      "Epoch  12 Batch  300/1077 - Train Accuracy: 0.9779, Validation Accuracy: 0.9766, Loss: 0.0348\n",
      "Epoch  12 Batch  600/1077 - Train Accuracy: 0.9757, Validation Accuracy: 0.9796, Loss: 0.0414\n",
      "Epoch  12 Batch  900/1077 - Train Accuracy: 0.9792, Validation Accuracy: 0.9826, Loss: 0.0476\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "            rnn1_loss_history.append(loss)\n",
    "            \n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "save_params(save_path,cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss history\n",
    "plt.plot(rnn1_loss_history, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change another type of RNN cell\n",
    "We are using LSTM cell as the original work, but GRU cell is getting more popular today, let's chage the cell type layer to GRU cell and see how it performs. Your parameters should be the same as above to compare the two units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "display_step = 300\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5\n",
    "grad_clip = 5\n",
    "cell_type = 'GRU' #THIS IS CHANGED TO GRU\n",
    "rnn2_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev_GRU'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int,\n",
    "                                                   cell_type)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        train_op = my_optimizer(cost,grad_clip,lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  300/1077 - Train Accuracy: 0.6102, Validation Accuracy: 0.6076, Loss: 1.1114\n",
      "Epoch   0 Batch  600/1077 - Train Accuracy: 0.7418, Validation Accuracy: 0.7283, Loss: 0.7418\n",
      "Epoch   0 Batch  900/1077 - Train Accuracy: 0.7413, Validation Accuracy: 0.7609, Loss: 0.6978\n",
      "Epoch   1 Batch  300/1077 - Train Accuracy: 0.7791, Validation Accuracy: 0.7839, Loss: 0.6039\n",
      "Epoch   1 Batch  600/1077 - Train Accuracy: 0.7943, Validation Accuracy: 0.7973, Loss: 0.5529\n",
      "Epoch   1 Batch  900/1077 - Train Accuracy: 0.7934, Validation Accuracy: 0.8043, Loss: 0.5393\n",
      "Epoch   2 Batch  300/1077 - Train Accuracy: 0.8173, Validation Accuracy: 0.8112, Loss: 0.4742\n",
      "Epoch   2 Batch  600/1077 - Train Accuracy: 0.8173, Validation Accuracy: 0.8247, Loss: 0.4454\n",
      "Epoch   2 Batch  900/1077 - Train Accuracy: 0.8260, Validation Accuracy: 0.8351, Loss: 0.4393\n",
      "Epoch   3 Batch  300/1077 - Train Accuracy: 0.8494, Validation Accuracy: 0.8433, Loss: 0.3878\n",
      "Epoch   3 Batch  600/1077 - Train Accuracy: 0.8477, Validation Accuracy: 0.8529, Loss: 0.3697\n",
      "Epoch   3 Batch  900/1077 - Train Accuracy: 0.8477, Validation Accuracy: 0.8477, Loss: 0.3835\n",
      "Epoch   4 Batch  300/1077 - Train Accuracy: 0.8737, Validation Accuracy: 0.8542, Loss: 0.3331\n",
      "Epoch   4 Batch  600/1077 - Train Accuracy: 0.8676, Validation Accuracy: 0.8724, Loss: 0.3203\n",
      "Epoch   4 Batch  900/1077 - Train Accuracy: 0.8780, Validation Accuracy: 0.8720, Loss: 0.3251\n",
      "Epoch   5 Batch  300/1077 - Train Accuracy: 0.8980, Validation Accuracy: 0.8728, Loss: 0.2800\n",
      "Epoch   5 Batch  600/1077 - Train Accuracy: 0.8954, Validation Accuracy: 0.8850, Loss: 0.2649\n",
      "Epoch   5 Batch  900/1077 - Train Accuracy: 0.8967, Validation Accuracy: 0.8850, Loss: 0.2876\n",
      "Epoch   6 Batch  300/1077 - Train Accuracy: 0.9036, Validation Accuracy: 0.8880, Loss: 0.2498\n",
      "Epoch   6 Batch  600/1077 - Train Accuracy: 0.9023, Validation Accuracy: 0.8980, Loss: 0.2530\n",
      "Epoch   6 Batch  900/1077 - Train Accuracy: 0.9093, Validation Accuracy: 0.9010, Loss: 0.2581\n",
      "Epoch   7 Batch  300/1077 - Train Accuracy: 0.9240, Validation Accuracy: 0.9093, Loss: 0.2253\n",
      "Epoch   7 Batch  600/1077 - Train Accuracy: 0.9188, Validation Accuracy: 0.9154, Loss: 0.2161\n",
      "Epoch   7 Batch  900/1077 - Train Accuracy: 0.9188, Validation Accuracy: 0.9193, Loss: 0.2338\n",
      "Epoch   8 Batch  300/1077 - Train Accuracy: 0.9280, Validation Accuracy: 0.9158, Loss: 0.2044\n",
      "Epoch   8 Batch  600/1077 - Train Accuracy: 0.9210, Validation Accuracy: 0.9197, Loss: 0.2028\n",
      "Epoch   8 Batch  900/1077 - Train Accuracy: 0.9249, Validation Accuracy: 0.9232, Loss: 0.2139\n",
      "Epoch   9 Batch  300/1077 - Train Accuracy: 0.9332, Validation Accuracy: 0.9201, Loss: 0.1988\n",
      "Epoch   9 Batch  600/1077 - Train Accuracy: 0.9327, Validation Accuracy: 0.9262, Loss: 0.1893\n",
      "Epoch   9 Batch  900/1077 - Train Accuracy: 0.9258, Validation Accuracy: 0.9232, Loss: 0.2138\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "            rnn2_loss_history.append(loss)\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "save_params(save_path,cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss history\n",
    "plt.plot(rnn2_loss_history, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate\n",
    "This will translate translate_sentence from French to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = \"GRU\" #change this to LSTM or GRU to traslate using appropriate model\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params(cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev_GRU\n",
      "Input\n",
      "  Word Ids:      [59, 39, 69, 50, 25, 117, 241, 350, 273, 28, 32, 69, 214, 285, 175, 163]\n",
      "  French Words: ['new', 'jersey', 'est', 'parfois', 'calme', 'pendant', \"l'\", 'automne', ',', 'et', 'il', 'est', 'neigeux', 'en', 'avril', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [149, 115, 151, 95, 60, 210, 106, 39, 216, 29, 151, 60, 180, 229, 212, 1]\n",
      "  English Words: new jersey is sometimes freezing during autumn , and it is freezing in april . <EOS>\n",
      "\n",
      "Correct translation\n",
      "  English Words: new jersey is sometimes quiet during autumn , and it is snowy in april .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "translate_sentence = side_by_side_sentences[0][1]\n",
    "correct_translation = side_by_side_sentences[0][0]\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  French Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  English Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n",
    "\n",
    "print('\\nCorrect translation')\n",
    "print('  English Words: {}'.format(correct_translation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare your result and list pros and cons of using GRU cells compared to LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "- GRUs retain the gradient without any memory loop by just exploiting the network.\n",
    "- Performance of GRU cells and LSTM cells is comparable in most cases  \n",
    "- GRU is computationally more efficient because of a simpler structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
